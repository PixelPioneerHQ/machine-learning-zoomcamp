{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Deep Learning Homework - Hair Type Classification\n",
    "\n",
    "## Machine Learning Zoomcamp 2025 - Module 8\n",
    "\n",
    "This notebook implements a CNN for binary classification of hair types (straight vs curly) using PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 1114] A dynamic link library (DLL) initialization routine failed. Error loading \"D:\\tien.nv12\\learn\\machine-learning-zoomcamp\\.venv\\Lib\\site-packages\\torch\\lib\\c10.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptim\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\tien.nv12\\learn\\machine-learning-zoomcamp\\.venv\\Lib\\site-packages\\torch\\__init__.py:281\u001b[39m\n\u001b[32m    277\u001b[39m                     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m    279\u001b[39m         kernel32.SetErrorMode(prev_error_mode)\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m     \u001b[43m_load_dll_libraries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    282\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m _load_dll_libraries\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_cuda_dep_paths\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m, lib_folder: \u001b[38;5;28mstr\u001b[39m, lib_name: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Libraries can either be in\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# path/nvidia/lib_folder/lib or\u001b[39;00m\n\u001b[32m    288\u001b[39m     \u001b[38;5;66;03m# path/nvidia/cuXX/lib (since CUDA 13.0) or\u001b[39;00m\n\u001b[32m    289\u001b[39m     \u001b[38;5;66;03m# path/lib_folder/lib\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\tien.nv12\\learn\\machine-learning-zoomcamp\\.venv\\Lib\\site-packages\\torch\\__init__.py:264\u001b[39m, in \u001b[36m_load_dll_libraries\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    260\u001b[39m     err = ctypes.WinError(last_error)\n\u001b[32m    261\u001b[39m     err.strerror += (\n\u001b[32m    262\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m Error loading \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m or one of its dependencies.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    263\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    266\u001b[39m     is_loaded = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mOSError\u001b[39m: [WinError 1114] A dynamic link library (DLL) initialization routine failed. Error loading \"D:\\tien.nv12\\learn\\machine-learning-zoomcamp\\.venv\\Lib\\site-packages\\torch\\lib\\c10.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import wget\n",
    "import zipfile\n",
    "import statistics\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reproducibility Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(\"‚úÖ Reproducibility seeds set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset\n",
    "url = \"https://github.com/SVizor42/ML_Zoomcamp/releases/download/straight-curly-data/data.zip\"\n",
    "\n",
    "if not os.path.exists(\"data.zip\"):\n",
    "    print(\"Downloading dataset...\")\n",
    "    wget.download(url, \"data.zip\")\n",
    "    print(\"\\nDownload completed!\")\n",
    "\n",
    "if not os.path.exists(\"data\"):\n",
    "    print(\"Extracting dataset...\")\n",
    "    with zipfile.ZipFile(\"data.zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall(\".\")\n",
    "    print(\"Extraction completed!\")\n",
    "else:\n",
    "    print(\"Dataset already exists!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HairCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN for hair type classification following homework specifications:\n",
    "    - Input: (3, 200, 200)\n",
    "    - Conv2d: 32 filters, kernel_size=(3,3), padding=0, stride=1, ReLU\n",
    "    - MaxPool2d: pool_size=(2,2)\n",
    "    - Flatten\n",
    "    - Linear: 64 neurons, ReLU\n",
    "    - Linear: 1 neuron (for binary classification)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(HairCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, \n",
    "                              kernel_size=3, padding=0, stride=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Calculate the size after conv and pooling\n",
    "        # Input: (3, 200, 200)\n",
    "        # After conv1: (32, 198, 198)  # 200-3+1 = 198\n",
    "        # After pool1: (32, 99, 99)    # 198/2 = 99\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(32 * 99 * 99, 64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Convolutional layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "print(\"‚úÖ Model architecture defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Question 1: Loss Function Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"QUESTION 1: Which loss function to use?\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Options:\")\n",
    "print(\"- nn.MSELoss()\")\n",
    "print(\"- nn.BCEWithLogitsLoss() ‚Üê CORRECT\")\n",
    "print(\"- nn.CrossEntropyLoss()\")\n",
    "print(\"- nn.CosineEmbeddingLoss()\")\n",
    "print()\n",
    "print(\"For binary classification, we should use:\")\n",
    "print(\"‚úì nn.BCEWithLogitsLoss() - Binary Cross Entropy with Logits\")\n",
    "print(\"This is appropriate for binary classification tasks.\")\n",
    "\n",
    "answer_q1 = \"nn.BCEWithLogitsLoss()\"\n",
    "print(f\"\\nüìù ANSWER Q1: {answer_q1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Question 2: Count Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"QUESTION 2: Total number of parameters\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Options: 896, 11214912, 15896912, 20073473\")\n",
    "print()\n",
    "\n",
    "# Create model and count parameters\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HairCNN().to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "# Try to use torchsummary for detailed breakdown\n",
    "try:\n",
    "    summary(model, input_size=(3, 200, 200))\n",
    "except Exception as e:\n",
    "    print(f\"torchsummary error: {e}\")\n",
    "\n",
    "# Manual calculation verification\n",
    "print(\"\\nManual calculation:\")\n",
    "conv1_params = (3 * 3 * 3 + 1) * 32\n",
    "fc1_params = (32 * 99 * 99 + 1) * 64\n",
    "fc2_params = (64 + 1) * 1\n",
    "print(f\"Conv1 parameters: {conv1_params:,}\")\n",
    "print(f\"FC1 parameters: {fc1_params:,}\")\n",
    "print(f\"FC2 parameters: {fc2_params:,}\")\n",
    "calculated_total = conv1_params + fc1_params + fc2_params\n",
    "print(f\"Total calculated: {calculated_total:,}\")\n",
    "\n",
    "answer_q2 = total_params\n",
    "print(f\"\\nüìù ANSWER Q2: {answer_q2:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Transformations and Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial transforms (without augmentation)\n",
    "train_transforms_initial = transforms.Compose([\n",
    "    transforms.Resize((200, 200)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )  # ImageNet normalization\n",
    "])\n",
    "\n",
    "# Test transforms (no augmentation)\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((200, 200)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# Load datasets without augmentation\n",
    "train_dataset = datasets.ImageFolder('data/train', transform=train_transforms_initial)\n",
    "test_dataset = datasets.ImageFolder('data/test', transform=test_transforms)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=20, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Classes: {train_dataset.classes}\")\n",
    "print(f\"Class to index mapping: {train_dataset.class_to_idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.002, momentum=0.8)\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Loss function: {criterion}\")\n",
    "print(f\"Optimizer: {optimizer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Initial Training (10 epochs without augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"INITIAL TRAINING (WITHOUT AUGMENTATION)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "num_epochs = 10\n",
    "history_initial = {'acc': [], 'loss': [], 'val_acc': [], 'val_loss': []}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        labels = labels.float().unsqueeze(1)  # Ensure labels are float and have shape (batch_size, 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        # For binary classification with BCEWithLogitsLoss, apply sigmoid to outputs before thresholding for accuracy\n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    epoch_acc = correct_train / total_train\n",
    "    history_initial['loss'].append(epoch_loss)\n",
    "    history_initial['acc'].append(epoch_acc)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            labels = labels.float().unsqueeze(1)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_running_loss += loss.item() * images.size(0)\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "    val_epoch_loss = val_running_loss / len(test_dataset)\n",
    "    val_epoch_acc = correct_val / total_val\n",
    "    history_initial['val_loss'].append(val_epoch_loss)\n",
    "    history_initial['val_acc'].append(val_epoch_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:2d}/{num_epochs}, \"\n",
    "          f\"Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}, \"\n",
    "          f\"Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Initial training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Question 3: Median Training Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"QUESTION 3: Median of training accuracy for all epochs\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Options: 0.05, 0.12, 0.40, 0.84\")\n",
    "print()\n",
    "\n",
    "print(\"Training accuracies for all 10 epochs:\")\n",
    "for i, acc in enumerate(history_initial['acc']):\n",
    "    print(f\"Epoch {i+1:2d}: {acc:.4f}\")\n",
    "\n",
    "median_acc = statistics.median(history_initial['acc'])\n",
    "print(f\"\\nMedian training accuracy: {median_acc:.4f}\")\n",
    "\n",
    "# Find closest option\n",
    "options_q3 = [0.05, 0.12, 0.40, 0.84]\n",
    "closest_q3 = min(options_q3, key=lambda x: abs(x - median_acc))\n",
    "print(f\"Closest option: {closest_q3}\")\n",
    "\n",
    "answer_q3 = closest_q3\n",
    "print(f\"\\nüìù ANSWER Q3: {answer_q3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Question 4: Standard Deviation of Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"QUESTION 4: Standard deviation of training loss for all epochs\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Options: 0.007, 0.078, 0.171, 1.710\")\n",
    "print()\n",
    "\n",
    "print(\"Training losses for all 10 epochs:\")\n",
    "for i, loss in enumerate(history_initial['loss']):\n",
    "    print(f\"Epoch {i+1:2d}: {loss:.4f}\")\n",
    "\n",
    "std_loss = statistics.stdev(history_initial['loss'])\n",
    "print(f\"\\nStandard deviation of training loss: {std_loss:.4f}\")\n",
    "\n",
    "# Find closest option\n",
    "options_q4 = [0.007, 0.078, 0.171, 1.710]\n",
    "closest_q4 = min(options_q4, key=lambda x: abs(x - std_loss))\n",
    "print(f\"Closest option: {closest_q4}\")\n",
    "\n",
    "answer_q4 = closest_q4\n",
    "print(f\"\\nüìù ANSWER Q4: {answer_q4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Data Augmentation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms with data augmentation\n",
    "train_transforms_augmented = transforms.Compose([\n",
    "    transforms.RandomRotation(50),\n",
    "    transforms.RandomResizedCrop(200, scale=(0.9, 1.0), ratio=(0.9, 1.1)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Resize((200, 200)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# Create new dataset with augmentation\n",
    "train_dataset_aug = datasets.ImageFolder('data/train', transform=train_transforms_augmented)\n",
    "train_loader_aug = DataLoader(train_dataset_aug, batch_size=20, shuffle=True)\n",
    "\n",
    "print(\"‚úÖ Data augmentation setup completed!\")\n",
    "print(\"Augmentations applied:\")\n",
    "print(\"- RandomRotation(50)\")\n",
    "print(\"- RandomResizedCrop(200, scale=(0.9, 1.0), ratio=(0.9, 1.1))\")\n",
    "print(\"- RandomHorizontalFlip()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Training with Data Augmentation (10 more epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"TRAINING WITH DATA AUGMENTATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Continue training with augmentation (don't recreate model!)\n",
    "num_epochs_aug = 10\n",
    "history_aug = {'acc': [], 'loss': [], 'val_acc': [], 'val_loss': []}\n",
    "\n",
    "print(\"Continuing training with data augmentation for 10 more epochs...\")\n",
    "\n",
    "for epoch in range(num_epochs_aug):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    for images, labels in train_loader_aug:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        labels = labels.float().unsqueeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataset_aug)\n",
    "    epoch_acc = correct_train / total_train\n",
    "    history_aug['loss'].append(epoch_loss)\n",
    "    history_aug['acc'].append(epoch_acc)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            labels = labels.float().unsqueeze(1)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_running_loss += loss.item() * images.size(0)\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "    val_epoch_loss = val_running_loss / len(test_dataset)\n",
    "    val_epoch_acc = correct_val / total_val\n",
    "    history_aug['val_loss'].append(val_epoch_loss)\n",
    "    history_aug['val_acc'].append(val_epoch_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:2d}/{num_epochs_aug}, \"\n",
    "          f\"Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}, \"\n",
    "          f\"Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Augmentation training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Question 5: Mean Test Loss with Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"QUESTION 5: Mean test loss for all epochs with augmentations\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Options: 0.008, 0.08, 0.88, 8.88\")\n",
    "print()\n",
    "\n",
    "print(\"Validation losses for all 10 epochs with augmentation:\")\n",
    "for i, loss in enumerate(history_aug['val_loss']):\n",
    "    print(f\"Epoch {i+1:2d}: {loss:.4f}\")\n",
    "\n",
    "mean_val_loss = statistics.mean(history_aug['val_loss'])\n",
    "print(f\"\\nMean validation loss: {mean_val_loss:.4f}\")\n",
    "\n",
    "# Find closest option\n",
    "options_q5 = [0.008, 0.08, 0.88, 8.88]\n",
    "closest_q5 = min(options_q5, key=lambda x: abs(x - mean_val_loss))\n",
    "print(f\"Closest option: {closest_q5}\")\n",
    "\n",
    "answer_q5 = closest_q5\n",
    "print(f\"\\nüìù ANSWER Q5: {answer_q5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Question 6: Average Test Accuracy for Last 5 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"QUESTION 6: Average test accuracy for last 5 epochs (6-10)\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Options: 0.08, 0.28, 0.68, 0.98\")\n",
    "print()\n",
    "\n",
    "print(\"Validation accuracies for all 10 epochs with augmentation:\")\n",
    "for i, acc in enumerate(history_aug['val_acc']):\n",
    "    print(f\"Epoch {i+1:2d}: {acc:.4f}\")\n",
    "\n",
    "# Get validation accuracy for epochs 6-10 (indices 5-9)\n",
    "last_5_acc = history_aug['val_acc'][5:]  # epochs 6-10\n",
    "print(f\"\\nLast 5 epochs (6-10) validation accuracies:\")\n",
    "for i, acc in enumerate(last_5_acc, 6):\n",
    "    print(f\"Epoch {i:2d}: {acc:.4f}\")\n",
    "\n",
    "avg_last_5_acc = statistics.mean(last_5_acc)\n",
    "print(f\"\\nAverage validation accuracy for last 5 epochs: {avg_last_5_acc:.4f}\")\n",
    "\n",
    "# Find closest option\n",
    "options_q6 = [0.08, 0.28, 0.68, 0.98]\n",
    "closest_q6 = min(options_q6, key=lambda x: abs(x - avg_last_5_acc))\n",
    "print(f\"Closest option: {closest_q6}\")\n",
    "\n",
    "answer_q6 = closest_q6\n",
    "print(f\"\\nüìù ANSWER Q6: {answer_q6}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Final Answers Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"HOMEWORK ANSWERS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Question 1: {answer_q1}\")\n",
    "print(f\"Question 2: {answer_q2:,}\")\n",
    "print(f\"Question 3: {answer_q3} (actual: {median_acc:.4f})\")\n",
    "print(f\"Question 4: {answer_q4} (actual: {std_loss:.4f})\")\n",
    "print(f\"Question 5: {answer_q5} (actual: {mean_val_loss:.4f})\")\n",
    "print(f\"Question 6: {answer_q6} (actual: {avg_last_5_acc:.4f})\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"üéØ Submit your answers here:\")\n",
    "print(\"https://courses.datatalks.club/ml-zoomcamp-2025/homework/hw08\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Model Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Training and Validation Loss (Initial)\n",
    "axes[0, 0].plot(range(1, 11), history_initial['loss'], 'b-', label='Training Loss')\n",
    "axes[0, 0].plot(range(1, 11), history_initial['val_loss'], 'r-', label='Validation Loss')\n",
    "axes[0, 0].set_title('Loss - Initial Training (No Augmentation)')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Plot 2: Training and Validation Accuracy (Initial)\n",
    "axes[0, 1].plot(range(1, 11), history_initial['acc'], 'b-', label='Training Accuracy')\n",
    "axes[0, 1].plot(range(1, 11), history_initial['val_acc'], 'r-', label='Validation Accuracy')\n",
    "axes[0, 1].set_title('Accuracy - Initial Training (No Augmentation)')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Plot 3: Training and Validation Loss (Augmentation)\n",
    "axes[1, 0].plot(range(1, 11), history_aug['loss'], 'b-', label='Training Loss')\n",
    "axes[1, 0].plot(range(1, 11), history_aug['val_loss'], 'r-', label='Validation Loss')\n",
    "axes[1, 0].set_title('Loss - Training with Augmentation')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Loss')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Plot 4: Training and Validation Accuracy (Augmentation)\n",
    "axes[1, 1].plot(range(1, 11), history_aug['acc'], 'b-', label='Training Accuracy')\n",
    "axes[1, 1].plot(range(1, 11), history_aug['val_acc'], 'r-', label='Validation Accuracy')\n",
    "axes[1, 1].set_title('Accuracy - Training with Augmentation')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Accuracy')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìà Key Observations:\")\n",
    "print(\"1. Initial training showed overfitting (high train acc, lower val acc)\")\n",
    "print(\"2. Data augmentation reduced overfitting\")\n",
    "print(\"3. Validation accuracy improved with augmentation\")\n",
    "print(\"4. Training and validation curves became more aligned with augmentation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
